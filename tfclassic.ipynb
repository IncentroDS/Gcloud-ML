{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 2b. Working with low-level TensorFlow </h1>\n",
    "\n",
    "This notebook is Lab2b of CPB 102, Google's course on Machine Learning using Cloud ML.\n",
    "\n",
    "In this notebook, we will work with relatively low-level TensorFlow functions to implement a linear regression model. We will use this notebook to demonstrate early stopping -- a technique whereby training is stopped once the error on the validation dataset starts to increase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datalab.bigquery as bq\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to read data and compute error is the same as Lab2a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickuplon</th>\n",
       "      <th>pickuplat</th>\n",
       "      <th>dropofflon</th>\n",
       "      <th>dropofflat</th>\n",
       "      <th>passengers</th>\n",
       "      <th>fare_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-73.984162</td>\n",
       "      <td>40.767241</td>\n",
       "      <td>-73.967796</td>\n",
       "      <td>40.752417</td>\n",
       "      <td>1</td>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-74.005099</td>\n",
       "      <td>40.719629</td>\n",
       "      <td>-74.010202</td>\n",
       "      <td>40.719718</td>\n",
       "      <td>3</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-74.004951</td>\n",
       "      <td>40.748075</td>\n",
       "      <td>-74.013482</td>\n",
       "      <td>40.715892</td>\n",
       "      <td>1</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.988091</td>\n",
       "      <td>40.733528</td>\n",
       "      <td>-73.939537</td>\n",
       "      <td>40.705488</td>\n",
       "      <td>3</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.970687</td>\n",
       "      <td>40.764815</td>\n",
       "      <td>-73.984393</td>\n",
       "      <td>40.764038</td>\n",
       "      <td>5</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pickuplon  pickuplat  dropofflon  dropofflat  passengers  fare_amount\n",
       "0 -73.984162  40.767241  -73.967796   40.752417           1          9.7\n",
       "1 -74.005099  40.719629  -74.010202   40.719718           3          5.3\n",
       "2 -74.004951  40.748075  -74.013482   40.715892           1          9.5\n",
       "3 -73.988091  40.733528  -73.939537   40.705488           3         17.5\n",
       "4 -73.970687  40.764815  -73.984393   40.764038           5          5.3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_dataset(filename):\n",
    "  return pd.read_csv(filename, header=None, names=['pickuplon','pickuplat','dropofflon','dropofflat','passengers','fare_amount'])\n",
    "\n",
    "df_train = read_dataset('../lab1a/taxi-train.csv')\n",
    "df_valid = read_dataset('../lab1a/taxi-valid.csv')\n",
    "df_test = read_dataset('../lab1a/taxi-test.csv')\n",
    "df_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FEATURE_COLS = np.arange(0,5)\n",
    "TARGET_COL   = 'fare_amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_rmse(actual, predicted):\n",
    "  return np.sqrt(np.mean((actual-predicted)**2))\n",
    "\n",
    "def print_rmse(model):\n",
    "  print \"Train RMSE = {0}\".format(compute_rmse(df_train[TARGET_COL], model.predict(df_train.iloc[:,FEATURE_COLS].values)))\n",
    "  print \"Valid RMSE = {0}\".format(compute_rmse(df_valid[TARGET_COL], model.predict(df_valid.iloc[:,FEATURE_COLS].values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Linear Regression </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1 train_error=9.28726752924 valid_err=13.157337064\n",
      "Model written to /tmp/trained_model-1\n",
      "iter=101 train_error=8.25731901078 valid_err=11.7068532873\n",
      "Model written to /tmp/trained_model-101\n",
      "iter=201 train_error=7.52286358458 valid_err=10.6714583268\n",
      "Model written to /tmp/trained_model-201\n",
      "iter=301 train_error=7.05536998721 valid_err=10.0105244801\n",
      "Model written to /tmp/trained_model-301\n",
      "iter=401 train_error=6.79287736641 valid_err=9.63729496012\n",
      "Model written to /tmp/trained_model-401\n",
      "iter=501 train_error=6.66392821009 valid_err=9.45201895956\n",
      "Model written to /tmp/trained_model-501\n",
      "iter=601 train_error=6.60867503714 valid_err=9.37107573261\n",
      "Model written to /tmp/trained_model-601\n",
      "iter=701 train_error=6.58803735302 valid_err=9.33971465387\n",
      "Model written to /tmp/trained_model-701\n",
      "iter=801 train_error=6.58130854042 valid_err=9.32876252809\n",
      "Model written to /tmp/trained_model-801\n",
      "iter=901 train_error=6.57939977869 valid_err=9.32520994839\n",
      "Model written to /tmp/trained_model-901\n",
      "iter=1001 train_error=6.57893213014 valid_err=9.32408660525\n",
      "Model written to /tmp/trained_model-1001\n",
      "iter=1101 train_error=6.57882606404 valid_err=9.32372047513\n",
      "Model written to /tmp/trained_model-1101\n",
      "iter=1201 train_error=6.57880864092 valid_err=9.32359402607\n",
      "Model written to /tmp/trained_model-1201\n",
      "iter=1301 train_error=6.5788068069 valid_err=9.32354901247\n",
      "Model written to /tmp/trained_model-1301\n",
      "iter=1401 train_error=6.57880466722 valid_err=9.32353363628\n",
      "Model written to /tmp/trained_model-1401\n",
      "iter=1501 train_error=6.57880191619 valid_err=9.32352920315\n",
      "Model written to /tmp/trained_model-1501\n",
      "iter=1601 train_error=6.5787988595 valid_err=9.32352873815\n",
      "Model written to /tmp/trained_model-1601\n",
      "iter=1701 train_error=6.57879733115 valid_err=9.32352962752\n",
      "Early stop!\n",
      "Error on Test data = 9.6654882769\n"
     ]
    }
   ],
   "source": [
    "predictors = df_train.iloc[:,FEATURE_COLS].values\n",
    "targets = df_train[TARGET_COL].values\n",
    "prev_valid_error = 10000 # huge number\n",
    "modelprefix = '/tmp/trained_model'\n",
    "with tf.Session() as sess:\n",
    "  npredictors = len(FEATURE_COLS)\n",
    "  noutputs = 1\n",
    "  feature_data = tf.placeholder(\"float\", [None, npredictors])\n",
    "  target_data = tf.placeholder(\"float\", [None, noutputs])\n",
    "  weights = tf.Variable(tf.truncated_normal([npredictors, noutputs], stddev=0.01))\n",
    "  biases = tf.Variable(tf.ones([noutputs]))\n",
    "  model = (tf.matmul(feature_data, weights) + biases) # LINEAR REGRESSION\n",
    "  cost = tf.nn.l2_loss(model - target_data) # Square Error, not RMSE\n",
    "  saver = tf.train.Saver({'weights' : weights, 'biases' : biases})\n",
    "    \n",
    "  training_step = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "  tf.initialize_all_variables().run()\n",
    "  for iter in xrange(0, 10000):\n",
    "    _, trainerr = sess.run([training_step, cost], feed_dict = {\n",
    "        feature_data : predictors,\n",
    "        target_data : targets.reshape(len(predictors), noutputs)\n",
    "      })\n",
    "    if (iter%100 == 1):\n",
    "      # early stop if validation error doesn't keep dropping\n",
    "      preds = sess.run(model, feed_dict = {feature_data : df_valid.iloc[:,FEATURE_COLS].values})\n",
    "      trmse = np.sqrt(trainerr/len(predictors))\n",
    "      vrmse = compute_rmse(df_valid[TARGET_COL], preds[0])      \n",
    "      print 'iter={0} train_error={1} valid_err={2}'.format(iter, trmse, vrmse)\n",
    "      if vrmse > prev_valid_error:\n",
    "         print \"Early stop!\"\n",
    "         break  # out of iteration loop\n",
    "      else:\n",
    "         prev_valid_error = vrmse\n",
    "         # save the model so that we can read it\n",
    "         modelfile = saver.save(sess, modelprefix, global_step=iter)\n",
    "         print 'Model written to {0}'.format(modelfile)\n",
    "\n",
    "  preds = sess.run(model, feed_dict = {feature_data : df_test.iloc[:,FEATURE_COLS].values})\n",
    "  testrmse = compute_rmse(df_test[TARGET_COL], preds[0]) \n",
    "  print 'Error on Test data = {0}'.format(testrmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the training error can be driven down very low, but it doesn't actually reduce the validation error.  To help prevent over-fitting, the loop above makes use of \"early-stopping\", to stop the training when the validation error starts to increase.  In tf.learn, we didn't pass in a validation dataset, but we got similar performance on the validation set -- that's because tf.learn uses a different technique called regularization to help prevent over-fitting.\n",
    "\n",
    "Early stopping and regularization are not that critical in linear regression (because the model is quite simple), but are crucial once you start creating deep neural networks where there are thousands of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
